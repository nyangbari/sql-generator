"""SQL Generation Service - With JOIN validation"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from config.prompts import SQL_GENERATION_PROMPT_TEMPLATE, ANSWER_PROMPT
from config.settings import MODEL_CONFIG, ANSWER_MODEL_CONFIG
import re

class SQLService:
    """SQL ìƒì„± ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        # SQLCoder ë¡œë“œ (SQL ìƒì„±ìš©)
        print("ğŸ”„ SQLCoder ë¡œë”©...")

        self.tokenizer = AutoTokenizer.from_pretrained(
            MODEL_CONFIG['model_id'],
            trust_remote_code=True
        )

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_CONFIG['model_id'],
            torch_dtype=torch.float16,
            device_map=MODEL_CONFIG['device_map'],
            load_in_8bit=MODEL_CONFIG['load_in_8bit'],
            trust_remote_code=True
        )

        print("âœ… SQLCoder ë¡œë“œ ì™„ë£Œ!")

        # Qwen2 ë¡œë“œ (ìì—°ì–´ ë‹µë³€ ìƒì„±ìš©)
        print("ğŸ”„ Qwen2 ë¡œë”©...")

        self.answer_tokenizer = AutoTokenizer.from_pretrained(
            ANSWER_MODEL_CONFIG['model_id'],
            trust_remote_code=True
        )

        self.answer_model = AutoModelForCausalLM.from_pretrained(
            ANSWER_MODEL_CONFIG['model_id'],
            torch_dtype=torch.float16,
            device_map=ANSWER_MODEL_CONFIG['device_map'],
            load_in_8bit=ANSWER_MODEL_CONFIG['load_in_8bit'],
            trust_remote_code=True
        )

        print("âœ… Qwen2 ë¡œë“œ ì™„ë£Œ!")

    def select_tables(self, question, candidates):
        """í›„ë³´ í…Œì´ë¸” ì¤‘ì—ì„œ í•„ìš”í•œ í…Œì´ë¸” ì„ íƒ (Qwen2 ì‚¬ìš©)

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            candidates: RAGê°€ ì„ íƒí•œ í›„ë³´ í…Œì´ë¸” ë¦¬ìŠ¤íŠ¸ [{name, schema, description, columns}, ...]

        Returns:
            list: ì„ íƒëœ í…Œì´ë¸” ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{name, schema}, ...]
        """
        # í›„ë³´ê°€ 2ê°œ ì´í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (Qwen2 ë¶ˆí•„ìš”)
        if len(candidates) <= 2:
            print(f"   â­ï¸  í›„ë³´ {len(candidates)}ê°œ - Qwen2 ìŠ¤í‚µ")
            return candidates

        try:
            # í…Œì´ë¸” ëª©ë¡ ìƒì„±
            table_list = []
            for c in candidates:
                desc = c.get('description', '')[:150]
                cols = ', '.join(c.get('columns', [])[:8])
                table_list.append(f"- {c['name']}\n  Purpose: {desc}\n  Columns: {cols}")

            tables_text = "\n\n".join(table_list)

            messages = [
                {"role": "user", "content": f"""You are a database expert. Select tables needed to answer the question.

Available tables:
{tables_text}

Question: {question}

Think step by step:
1. What data does the question ask for?
2. Which tables have the relevant columns?
3. Check column comments for status/type meanings.

Return ONLY the table names needed, one per line:"""}
            ]

            prompt = self.answer_tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            inputs = self.answer_tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=4096
            )

            inputs = inputs.to(self.answer_model.device)

            with torch.no_grad():
                outputs = self.answer_model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.1,  # ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± í™•ë³´
                    do_sample=True,
                    pad_token_id=self.answer_tokenizer.eos_token_id,
                    use_cache=False,
                )

            response = self.answer_tokenizer.decode(outputs[0], skip_special_tokens=False)

            # Qwen2 ì‘ë‹µ ì¶”ì¶œ
            if "<|im_start|>assistant" in response:
                answer = response.split("<|im_start|>assistant")[-1]
                if "<|im_end|>" in answer:
                    answer = answer.split("<|im_end|>")[0]
                answer = answer.strip()
            else:
                response_clean = self.answer_tokenizer.decode(outputs[0], skip_special_tokens=True)
                if "one per line:" in response_clean:
                    answer = response_clean.split("one per line:")[-1].strip()
                else:
                    answer = response_clean[-200:].strip()

            print(f"   ğŸ“ Qwen2 raw output: {answer[:200]}")  # ë””ë²„ê·¸

            # í…Œì´ë¸” ì´ë¦„ íŒŒì‹± (ì¤„ë°”ê¿ˆ, ì‰¼í‘œ ëª¨ë‘ ì²˜ë¦¬)
            candidate_names = {c['name'].lower(): c for c in candidates}
            selected = []

            # ì¤„ë°”ê¿ˆê³¼ ì‰¼í‘œë¡œ ë¶„ë¦¬
            parts = answer.replace(',', '\n').split('\n')
            for part in parts:
                name = part.strip().lower()
                if name in candidate_names and candidate_names[name] not in selected:
                    selected.append(candidate_names[name])

            print(f"   ğŸ¤– Qwen2 ì„ íƒ: {[t['name'] for t in selected]}")

            # ì„ íƒëœ ê²Œ ì—†ìœ¼ë©´ ìƒìœ„ 3ê°œ ë°˜í™˜
            return selected if selected else candidates[:3]

        except Exception as e:
            print(f"   âš ï¸  í…Œì´ë¸” ì„ íƒ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return candidates[:3]

    def generate(self, question, tables, hints=None, db_type="MySQL"):
        """SQL ìƒì„±"""
        try:
            enhanced_question = question
            if hints:
                hint_text = " ".join(hints)
                enhanced_question = f"{question} ({hint_text})"

            schema = "\n\n".join([t["schema"] for t in tables])

            prompt = SQL_GENERATION_PROMPT_TEMPLATE.format(
                db_type=db_type,
                question=enhanced_question,
                schema=schema
            )

            prompt_text = str(prompt).strip()

            inputs = self.tokenizer.encode(
                prompt_text,
                return_tensors="pt",
                truncation=True,
                max_length=2048,
                add_special_tokens=True
            )

            inputs = inputs.to(self.model.device)

            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=MODEL_CONFIG['max_new_tokens'],
                    min_new_tokens=MODEL_CONFIG.get('min_new_tokens', 20),
                    temperature=MODEL_CONFIG['temperature'],
                    top_p=MODEL_CONFIG.get('top_p', 0.9),
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    num_beams=1,
                    early_stopping=False,
                    repetition_penalty=1.1
                )

            print(f"   ğŸ“Š Generated {outputs.shape[1] - inputs.shape[1]} new tokens")

            result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            if result.startswith(prompt_text):
                new_content = result[len(prompt_text):].strip()
            else:
                new_content = result

            sql = self._extract_sql(new_content if new_content else result)

            # Auto-fix MySQL issues
            if db_type.upper() == "MYSQL":
                sql = self._mysql_fix(sql)

            return sql

        except Exception as e:
            print(f"   âŒ Error: {e}")
            return f"SELECT * FROM {tables[0]['name']} LIMIT 10"
    
    def _extract_sql(self, text):
        """SQL ì¶”ì¶œ"""
        pattern = r'SELECT.+?FROM.+?(?:WHERE.+?)?(?:;|\n\n|$)'
        matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)

        if not matches:
            raise ValueError("No SELECT found")

        sql = max(matches, key=len)
        sql = sql.replace(';', '').strip()
        sql = re.sub(r'\s+', ' ', sql)

        return sql

    def _mysql_fix(self, sql):
        """PostgreSQL â†’ MySQL auto-fix"""
        original = sql

        sql = re.sub(r'\s+NULLS\s+(FIRST|LAST)', '', sql, flags=re.IGNORECASE)

        offset_match = re.search(r'LIMIT\s+(\d+)\s+OFFSET\s+(\d+)', sql, re.IGNORECASE)
        if offset_match:
            limit = offset_match.group(1)
            offset = offset_match.group(2)
            sql = re.sub(r'LIMIT\s+\d+\s+OFFSET\s+\d+', f'LIMIT {offset}, {limit}', sql, flags=re.IGNORECASE)

        sql = re.sub(r'::\w+', '', sql)
        sql = re.sub(r'\bILIKE\b', 'LIKE', sql, flags=re.IGNORECASE)

        if sql != original:
            print(f"   ğŸ”§ Auto-fixed for MySQL")

        return sql

    def generate_answer(self, question, sql_result):
        """SQL ê²°ê³¼ë¥¼ ìì—°ì–´ë¡œ ë³€í™˜ (Qwen2 ì‚¬ìš©)"""
        try:
            # Qwen2 chat format
            messages = [
                {"role": "user", "content": f"""ë‹¤ìŒ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ê²°ê³¼:
{sql_result}

ê°„ë‹¨í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš” (1-2ë¬¸ì¥). í•µì‹¬ë§Œ ë§í•´ì£¼ì„¸ìš”."""}
            ]

            # Qwen2 chat template ì ìš©
            prompt = self.answer_tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            inputs = self.answer_tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=2048
            )

            inputs = inputs.to(self.answer_model.device)

            with torch.no_grad():
                outputs = self.answer_model.generate(
                    **inputs,
                    max_new_tokens=ANSWER_MODEL_CONFIG['max_new_tokens'],
                    temperature=ANSWER_MODEL_CONFIG['temperature'],
                    do_sample=True,
                    pad_token_id=self.answer_tokenizer.eos_token_id,
                    use_cache=False,
                )

            # Qwen2: skip_special_tokens=Falseë¡œ ë§ˆì»¤ ìœ ì§€
            response = self.answer_tokenizer.decode(outputs[0], skip_special_tokens=False)

            print(f"   ğŸ“ Qwen2 raw: {response[-300:]}")  # ë””ë²„ê·¸

            # Qwen2 ì‘ë‹µ ì¶”ì¶œ (ë§ˆì»¤: <|im_start|>assistant ... <|im_end|>)
            if "<|im_start|>assistant" in response:
                # assistant ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                answer = response.split("<|im_start|>assistant")[-1]
                # ë ë§ˆì»¤ ì œê±°
                if "<|im_end|>" in answer:
                    answer = answer.split("<|im_end|>")[0]
                answer = answer.strip()
            else:
                # íŠ¹ìˆ˜ í† í° ì—†ì´ ë””ì½”ë”© í›„ í”„ë¡¬í”„íŠ¸ ì œê±°
                response_clean = self.answer_tokenizer.decode(outputs[0], skip_special_tokens=True)
                # í”„ë¡¬í”„íŠ¸ì—ì„œ user ë©”ì‹œì§€ ë¶€ë¶„ ì°¾ì•„ì„œ ê·¸ ì´í›„ë§Œ ì¶”ì¶œ
                if "í•µì‹¬ë§Œ ë§í•´ì£¼ì„¸ìš”." in response_clean:
                    answer = response_clean.split("í•µì‹¬ë§Œ ë§í•´ì£¼ì„¸ìš”.")[-1].strip()
                else:
                    answer = response_clean[-200:].strip()

            # ì²« ë¬¸ì¥ë§Œ (ê¹”ë”í•˜ê²Œ)
            answer = answer.split('\n')[0].strip()

            # ë¶ˆí•„ìš”í•œ ë§ˆì»¤ ì •ë¦¬
            answer = answer.replace("<|im_end|>", "").replace("<|im_start|>", "").strip()

            return answer

        except Exception as e:
            print(f"   âš ï¸  ìì—°ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
