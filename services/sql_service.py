"""SQL Generation Service"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from config.prompts import SQL_GENERATION_PROMPT
from config.settings import MODEL_CONFIG

class SQLService:
    """SQL ìƒì„± ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        print("ğŸ”„ SQLCoder ë¡œë”©...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['model_id'])
        
        # pad_tokenì´ ì—†ìœ¼ë©´ eos_tokenìœ¼ë¡œ ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_CONFIG['model_id'],
            torch_dtype=torch.float16,
            device_map=MODEL_CONFIG['device_map'],
            load_in_8bit=MODEL_CONFIG['load_in_8bit']
        )
        
        self.model = model
        
        print("âœ… SQLCoder ë¡œë“œ ì™„ë£Œ!")
    
    def generate(self, question, tables):
        """SQL ìƒì„±"""
        schema = "\n\n".join([t["schema"] for t in tables])
        
        prompt = SQL_GENERATION_PROMPT.format(
            question=question,
            schema=schema
        )
        
        # ğŸ¯ í•µì‹¬: promptë¥¼ ëª…ì‹œì ìœ¼ë¡œ strë¡œ ë³€í™˜í•˜ê³  ë‹¨ì¼ ì¸ìë¡œ ì „ë‹¬
        prompt_text = str(prompt).strip()
        
        # tokenizer í˜¸ì¶œ (ìµœì‹  ë²„ì „ í˜¸í™˜)
        inputs = self.tokenizer(
            prompt_text,  # â† str ë³´ì¥
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048,
            return_attention_mask=True
        )
        
        # GPUë¡œ ì´ë™
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_new_tokens=MODEL_CONFIG['max_new_tokens'],
                temperature=MODEL_CONFIG['temperature'],
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # SQL ì¶”ì¶œ
        sql = self._extract_sql(result)
        
        return sql
    
    def _extract_sql(self, result):
        """ìƒì„± ê²°ê³¼ì—ì„œ SQL ì¶”ì¶œ"""
        if "```sql" in result:
            sql = result.split("```sql")[-1].split("```")[0].strip()
        else:
            after_answer = result.split("### Answer")[-1]
            lines = after_answer.strip().split('\n')
            sql_lines = []
            for line in lines:
                if line.strip().upper().startswith('SELECT') or sql_lines:
                    sql_lines.append(line)
                    if ';' in line:
                        break
            sql = '\n'.join(sql_lines).strip()
        
        sql = sql.replace('```sql', '').replace('```', '').strip()
        if ';' in sql:
            sql = sql.split(';')[0].strip()
        
        return sql
