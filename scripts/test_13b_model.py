#!/usr/bin/env python3
# test_13b_model.py
# 13B ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆ)

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

print("="*70)
print("ğŸ§ª 13B ëª¨ë¸ í…ŒìŠ¤íŠ¸")
print("="*70)

# ì¶”ì²œ ëª¨ë¸ë“¤
models = {
    "1": "codellama/CodeLlama-13b-Instruct-hf",  # SQL íŠ¹í™”
    "2": "mistralai/Mistral-7B-Instruct-v0.2",   # 7Bì§€ë§Œ ê°•ë ¥
    "3": "meta-llama/Meta-Llama-3-8B-Instruct",  # ë²”ìš©
}

print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
for key, model in models.items():
    print(f"  {key}. {model}")

choice = input("\nì„ íƒ (1-3, ê¸°ë³¸ê°’ 1): ").strip() or "1"
model_id = models.get(choice, models["1"])

print(f"\nğŸ”„ ëª¨ë¸ ë¡œë”©: {model_id}")
print("   (ì²˜ìŒ ë‹¤ìš´ë¡œë“œ ì‹œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...)")

try:
    # GPU ì²´í¬
    if not torch.cuda.is_available():
        print("âŒ CUDA ì—†ìŒ! CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ë§¤ìš° ëŠë¦¼)")
    else:
        print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
        print(f"âœ… VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    # ëª¨ë¸ ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    print("\nğŸ”„ 8-bit ì–‘ìí™”ë¡œ ë¡œë”©...")
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto",
        load_in_8bit=True
    )
    
    # VRAM ì‚¬ìš©ëŸ‰ ì²´í¬
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        print(f"\nğŸ’¾ VRAM ì‚¬ìš©:")
        print(f"   í• ë‹¹ë¨: {allocated:.2f}GB")
        print(f"   ì˜ˆì•½ë¨: {reserved:.2f}GB")
    
    print("\nâœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    
    # í…ŒìŠ¤íŠ¸ 1: ì‹¤ì œ ìŠ¤í‚¤ë§ˆë¡œ í…ŒìŠ¤íŠ¸
    print("\n" + "="*70)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ 1: ë¯¸ì…˜ ê°œìˆ˜")
    print("="*70)
    
    test_prompt_1 = """Given these tables from knightfury database:

Table: fury_users
Columns: address, chainId, network, referralCode, username, isAdmin, telegramId, discordId, twitterId

Table: fury_mission_configs  
Columns: missionId, missionName, missionType, missionGroup, missionDetail, params, points

Table: fury_project_missions
Columns: id, projectId, missionId, isActive

Question: How many missions are in fury_mission_configs?

SQL:"""
    
    print(test_prompt_1)
    print("\nğŸ¤” ìƒì„± ì¤‘...")
    
    inputs = tokenizer(test_prompt_1, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    sql = result.split("SQL:")[-1].strip().split('\n')[0]
    
    print(f"\nğŸ’¾ ìƒì„±ëœ SQL:")
    print(sql)
    
    # ì²´í¬í¬ì¸íŠ¸
    correct_table = "fury_mission_configs" in sql.lower()
    has_from = "from" in sql.lower()
    
    print(f"\nâœ… ì²´í¬:")
    print(f"   FROM ì ˆ ìˆìŒ: {has_from}")
    print(f"   ì˜¬ë°”ë¥¸ í…Œì´ë¸”: {correct_table}")
    
    # í…ŒìŠ¤íŠ¸ 2: í…Œì´ë¸” ì„ íƒ í…ŒìŠ¤íŠ¸
    print("\n" + "="*70)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ 2: ë³µì¡í•œ í…Œì´ë¸” ì„ íƒ")
    print("="*70)
    
    test_prompt_2 = """Given these tables:

Table: fury_users
Columns: address, username, isAdmin

Table: fury_mission_configs
Columns: missionId, missionName, points

Table: fury_airdrop_projects
Columns: projectId, projectName, totalSupply

Table: fury_play_games
Columns: gameId, gameName, maxScore

Question: ì–¼ë§ˆë‚˜ ë§ì€ í”„ë¡œì íŠ¸ê°€ ìˆì–´?

SQL:"""
    
    print(test_prompt_2)
    print("\nğŸ¤” ìƒì„± ì¤‘...")
    
    inputs = tokenizer(test_prompt_2, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    sql = result.split("SQL:")[-1].strip().split('\n')[0]
    
    print(f"\nğŸ’¾ ìƒì„±ëœ SQL:")
    print(sql)
    
    # ì²´í¬í¬ì¸íŠ¸
    correct_table = "fury_airdrop_projects" in sql.lower()
    has_from = "from" in sql.lower()
    
    print(f"\nâœ… ì²´í¬:")
    print(f"   FROM ì ˆ ìˆìŒ: {has_from}")
    print(f"   ì˜¬ë°”ë¥¸ í…Œì´ë¸” (fury_airdrop_projects): {correct_table}")
    
    print("\n" + "="*70)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*70)
    
    # ê²°ê³¼ í‰ê°€
    print("\nğŸ“Š í‰ê°€:")
    if has_from and correct_table:
        print("   âœ… ì´ ëª¨ë¸ì€ 7Bë³´ë‹¤ í›¨ì”¬ ì¢‹ìŠµë‹ˆë‹¤!")
        print("   âœ… LangChainì— ì‚¬ìš© ê°€ëŠ¥!")
    elif has_from:
        print("   âš ï¸  FROM ì ˆì€ ìˆì§€ë§Œ í…Œì´ë¸” ì„ íƒì´ ë¶€ì •í™•")
        print("   âš ï¸  7Bë³´ë‹¤ëŠ” ë‚˜ì„ ìˆ˜ ìˆìŒ")
    else:
        print("   âŒ 7Bì™€ ë¹„ìŠ·í•œ ë¬¸ì œ ë°œìƒ")
        print("   âŒ ë” í° ëª¨ë¸ í•„ìš”")
    
except Exception as e:
    print(f"\nâŒ ì˜¤ë¥˜: {e}")
    
    if "out of memory" in str(e).lower():
        print("\nğŸ’¡ VRAM ë¶€ì¡±!")
        print("   í•´ê²°ì±…: 4-bit ì–‘ìí™” ì‹œë„")
        print("   ì½”ë“œ: load_in_4bit=True")
    else:
        print("\nğŸ’¡ í•´ê²°ì±…:")
        print("   1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ â†’ ì¸í„°ë„· ì—°ê²° í™•ì¸")
        print("   2. CUDA ì˜¤ë¥˜ â†’ GPU ë“œë¼ì´ë²„ í™•ì¸")
    
    import traceback
    traceback.print_exc()
