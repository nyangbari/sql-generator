# train_company_on_spider.py
# Spider ëª¨ë¸ ìœ„ì— íšŒì‚¬ ë°ì´í„° ì¶”ê°€ í•™ìŠµ

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import Dataset
import json
import time
from datetime import datetime

print("="*70)
print("ğŸš€ Spider + íšŒì‚¬ ë°ì´í„° í†µí•© í•™ìŠµ")
print("="*70)

# Configuration
MODEL_NAME = "codellama/CodeLlama-7b-Instruct-hf"
SPIDER_MODEL = "../models/sql-generator-full"  # â† Spider ëª¨ë¸!
OUTPUT_DIR = "../models/sql-generator-spider-plus-company"
DATA_DIR = "../data"

BATCH_SIZE = 1
GRADIENT_ACCUMULATION = 4
EPOCHS = 3  # íšŒì‚¬ ë°ì´í„°ëŠ” ì ìœ¼ë‹ˆê¹Œ 3 ì—í­
LEARNING_RATE = 5e-5  # ë‚®ê²Œ! (ê¸°ì¡´ ì§€ì‹ ìœ ì§€)
MAX_LENGTH = 512

# Device
device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"âœ… Device: {device}")

# ========================================
# 1. Spider ëª¨ë¸ ë¡œë“œ (í•µì‹¬!)
# ========================================

print(f"\nğŸ”„ Step 1: Loading Spider-trained model...")

# ë² ì´ìŠ¤ ëª¨ë¸
base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map=device,
    low_cpu_mem_usage=True
)

# Spider LoRA ë¡œë“œ
spider_model = PeftModel.from_pretrained(
    base_model,
    SPIDER_MODEL
)

print(f"âœ… Spider model loaded!")
print(f"ğŸ“š This model knows 7000 SQL examples from Spider")

# ========================================
# 2. ì¶”ê°€ í•™ìŠµ ì¤€ë¹„
# ========================================

print(f"\nğŸ”„ Step 2: Preparing for additional training...")

# Spider LoRAë¥¼ ë² ì´ìŠ¤ì— ë³‘í•© (ê¹”ë”í•œ ë°©ë²•)
model = spider_model.merge_and_unload()

# ìƒˆë¡œìš´ LoRA ë ˆì´ì–´ ì¶”ê°€ (íšŒì‚¬ ë°ì´í„°ìš©)
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# ========================================
# 3. íšŒì‚¬ ë°ì´í„° ë¡œë“œ
# ========================================

print(f"\nğŸ“š Loading company data...")

with open(f'{DATA_DIR}/company_train_regenerated.json', 'r', encoding='utf-8') as f:
    train_data = json.load(f)

with open(f'{DATA_DIR}/company_val_regenerated.json', 'r', encoding='utf-8') as f:
    val_data = json.load(f)

print(f"- Train: {len(train_data)} examples")
print(f"- Validation: {len(val_data)} examples")

# ========================================
# 4. ë°ì´í„° ì „ì²˜ë¦¬
# ========================================

def preprocess_function(example):
    full_text = example['input'] + "\n" + example['output'] + tokenizer.eos_token
    
    tokenized = tokenizer(
        full_text,
        truncation=True,
        max_length=MAX_LENGTH,
        padding="max_length"
    )
    
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

train_dataset = Dataset.from_list(train_data)
val_dataset = Dataset.from_list(val_data)

train_dataset = train_dataset.map(
    preprocess_function,
    remove_columns=train_dataset.column_names,
    desc="Processing train"
)

val_dataset = val_dataset.map(
    preprocess_function,
    remove_columns=val_dataset.column_names,
    desc="Processing val"
)

print(f"âœ… Dataset prepared!")

# ========================================
# 5. í•™ìŠµ ì„¤ì •
# ========================================

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    learning_rate=LEARNING_RATE,  # â† ë‚®ì€ LR!
    warmup_steps=50,
    fp16=True,
    optim="adamw_torch",
    max_grad_norm=0.5,  # â† ì‘ê²Œ (ê¸°ì¡´ ì§€ì‹ ë³´í˜¸)
    eval_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=3,
    load_best_model_at_end=True,
    logging_steps=20,
    logging_dir=f"{OUTPUT_DIR}/logs",
    report_to="none",
    dataloader_num_workers=0,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# ========================================
# 6. í•™ìŠµ ì‹œì‘!
# ========================================

total_steps = (len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION)) * EPOCHS

print("\n" + "="*70)
print("ğŸš€ Starting INCREMENTAL training...")
print("="*70)
print(f"""
ì´ ëª¨ë¸ì€:
âœ… Spider 7000ê°œ SQL (ì´ë¯¸ í•™ìŠµë¨)
â• íšŒì‚¬ {len(train_data)}ê°œ ë°ì´í„° (ì§€ê¸ˆ ì¶”ê°€)

= Spider + íšŒì‚¬ í†µí•© ëª¨ë¸

Total steps: ~{total_steps}
Learning rate: {LEARNING_RATE} (ë‚®ê²Œ ì„¤ì • - ê¸°ì¡´ ì§€ì‹ ìœ ì§€)

â±ï¸ Estimated time: 1-2 hours (MPS)
""")

print("Starting in 3 seconds...")
time.sleep(3)

try:
    start_time = time.time()
    
    print("\nğŸš€ Training started!\n")
    trainer.train()
    
    elapsed = time.time() - start_time
    
    print("\n" + "="*70)
    print("âœ… Training complete!")
    print("="*70)
    print(f"â±ï¸ Total time: {elapsed/3600:.1f} hours")
    
    # Save
    print(f"\nğŸ’¾ Saving integrated model...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"âœ… Saved to: {OUTPUT_DIR}")
    
    # Eval
    eval_results = trainer.evaluate()
    
    print(f"\nFinal results:")
    for key, value in eval_results.items():
        print(f"  {key}: {value:.4f}")
    
    # Save results
    with open(f"{OUTPUT_DIR}/training_results.txt", 'w') as f:
        f.write(f"Spider + Company Integrated Model\n")
        f.write(f"="*50 + "\n\n")
        f.write(f"Base: Spider model (7000 SQL examples)\n")
        f.write(f"Added: {len(train_data)} company examples\n")
        f.write(f"Training time: {elapsed/3600:.1f} hours\n\n")
        f.write(f"Final evaluation:\n")
        for key, value in eval_results.items():
            f.write(f"  {key}: {value:.4f}\n")
    
    print("\n" + "="*70)
    print("ğŸ‰ All done!")
    print("="*70)
    print(f"""
í†µí•© ëª¨ë¸ ì €ì¥ ì™„ë£Œ!
- Location: {OUTPUT_DIR}
- Spider 7000ê°œ + íšŒì‚¬ {len(train_data)}ê°œ

ì´ì œ ì´ ëª¨ë¸ì€:
âœ… ì˜ì–´ SQL (Spider)
âœ… í•œêµ­ì–´ ì§ˆë¬¸ (íšŒì‚¬)
ë‘˜ ë‹¤ ì˜ ì´í•´í•©ë‹ˆë‹¤!

í…ŒìŠ¤íŠ¸: python test_integrated_model.py
    """)

except KeyboardInterrupt:
    print("\nâš ï¸ Interrupted")
    trainer.save_model(f"{OUTPUT_DIR}/interrupted")

except Exception as e:
    print(f"\nâŒ Error: {e}")
    import traceback
    traceback.print_exc()
