#!/usr/bin/env python3
# test_13b_4bit.py
# 13B ëª¨ë¸ 4-bit ì–‘ìí™” í…ŒìŠ¤íŠ¸

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

print("="*70)
print("ğŸ§ª 13B ëª¨ë¸ í…ŒìŠ¤íŠ¸ (4-bit ì–‘ìí™”)")
print("="*70)

# 4-bit ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

model_id = "codellama/CodeLlama-13b-Instruct-hf"

print(f"\nğŸ”„ ëª¨ë¸ ë¡œë”©: {model_id}")
print("   ì–‘ìí™”: 4-bit NF4")

try:
    # GPU ì²´í¬
    if torch.cuda.is_available():
        print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
        print(f"âœ… VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # ëª¨ë¸ ë¡œë“œ (4-bit)
    print("\nğŸ”„ 4-bit ì–‘ìí™”ë¡œ ë¡œë”©...")
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto"
    )
    
    # VRAM ì‚¬ìš©ëŸ‰
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        print(f"\nğŸ’¾ VRAM ì‚¬ìš©:")
        print(f"   í• ë‹¹ë¨: {allocated:.2f}GB")
        print(f"   ì˜ˆì•½ë¨: {reserved:.2f}GB")
    
    print("\nâœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    
    # í…ŒìŠ¤íŠ¸ 1
    print("\n" + "="*70)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ 1: ë¯¸ì…˜ ê°œìˆ˜")
    print("="*70)
    
    test_prompt = """Given these tables:

Table: fury_users
Columns: address, username, isAdmin

Table: fury_mission_configs
Columns: missionId, missionName, points

Table: fury_airdrop_projects
Columns: projectId, projectName

Question: How many missions are in fury_mission_configs?

SQL:"""
    
    print(test_prompt)
    print("\nğŸ¤” ìƒì„± ì¤‘...")
    
    inputs = tokenizer(test_prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    sql = result.split("SQL:")[-1].strip().split('\n')[0]
    
    print(f"\nğŸ’¾ ìƒì„±ëœ SQL:")
    print(sql)
    
    # ì²´í¬
    has_from = "from" in sql.lower()
    correct_table = "fury_mission_configs" in sql.lower()
    
    print(f"\nâœ… ì²´í¬:")
    print(f"   FROM ì ˆ: {has_from}")
    print(f"   ì˜¬ë°”ë¥¸ í…Œì´ë¸”: {correct_table}")
    
    # í…ŒìŠ¤íŠ¸ 2
    print("\n" + "="*70)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ 2: í•œê¸€ + í…Œì´ë¸” ì„ íƒ")
    print("="*70)
    
    test_prompt2 = """Given these tables:

Table: fury_users
Columns: address, username

Table: fury_mission_configs
Columns: missionId, missionName

Table: fury_airdrop_projects
Columns: projectId, projectName, totalSupply

Question: ì–¼ë§ˆë‚˜ ë§ì€ í”„ë¡œì íŠ¸ê°€ ìˆì–´?

SQL:"""
    
    print(test_prompt2)
    print("\nğŸ¤” ìƒì„± ì¤‘...")
    
    inputs = tokenizer(test_prompt2, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    sql = result.split("SQL:")[-1].strip().split('\n')[0]
    
    print(f"\nğŸ’¾ ìƒì„±ëœ SQL:")
    print(sql)
    
    # ì²´í¬
    has_from = "from" in sql.lower()
    correct_table = "fury_airdrop_projects" in sql.lower()
    
    print(f"\nâœ… ì²´í¬:")
    print(f"   FROM ì ˆ: {has_from}")
    print(f"   ì˜¬ë°”ë¥¸ í…Œì´ë¸” (projects): {correct_table}")
    
    print("\n" + "="*70)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*70)
    
    # í‰ê°€
    print("\nğŸ“Š í‰ê°€:")
    if has_from and correct_table:
        print("   âœ… 13B ëª¨ë¸ì´ 7Bë³´ë‹¤ í›¨ì”¬ ì¢‹ìŠµë‹ˆë‹¤!")
        print("   âœ… 4-bit ì–‘ìí™”ë¡œë„ ì˜ ì‘ë™!")
        print("   âœ… LangChainì— ì‚¬ìš© ê°€ëŠ¥!")
    elif has_from:
        print("   âš ï¸  FROM ì ˆì€ ìˆì§€ë§Œ í…Œì´ë¸” ì„ íƒ ë¶€ì •í™•")
        print("   âš ï¸  Fine-tuning ê³ ë ¤")
    else:
        print("   âŒ 7Bì™€ ë¹„ìŠ·í•œ ë¬¸ì œ")
        print("   âŒ Fine-tuning í•„ìš”")
    
except Exception as e:
    print(f"\nâŒ ì˜¤ë¥˜: {e}")
    import traceback
    traceback.print_exc()
