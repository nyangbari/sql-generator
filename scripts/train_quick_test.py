# train_quick_test.py

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model
from datasets import Dataset
import json

print("="*70)
print("üöÄ SQL Generator - Quick Test (Small Dataset)")
print("="*70)

# ========================================
# Configuration - OPTIMIZED FOR SPEED
# ========================================

MODEL_NAME = "codellama/CodeLlama-7b-Instruct-hf"
OUTPUT_DIR = "../models/sql-generator-test"
DATA_DIR = "../data"

BATCH_SIZE = 1              # ÏûëÍ≤å
GRADIENT_ACCUMULATION = 4   # ÏûëÍ≤å
EPOCHS = 1                  # 1 ÏóêÌè≠Îßå
LEARNING_RATE = 2e-4
MAX_LENGTH = 256            # ÏßßÍ≤å

# ========================================
# 1. Check device
# ========================================

if torch.backends.mps.is_available():
    device = "mps"
    print(f"\n‚úÖ Using MPS (Apple Silicon GPU)")
else:
    device = "cpu"
    print(f"\n‚ö†Ô∏è Using CPU")

# ========================================
# 2. Load SMALL data
# ========================================

print(f"\nüìö Loading SMALL dataset...")

with open(f'{DATA_DIR}/train_small.json', 'r') as f:
    train_data = json.load(f)

with open(f'{DATA_DIR}/val_small.json', 'r') as f:
    val_data = json.load(f)

print(f"- Train: {len(train_data)} examples (small sample)")
print(f"- Validation: {len(val_data)} examples")

# ========================================
# 3. Load model and tokenizer
# ========================================

print(f"\nüîÑ Loading model...")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map=device,
    low_cpu_mem_usage=True
)

print(f"‚úÖ Model loaded!")

# ========================================
# 4. LoRA configuration - SMALLER
# ========================================

print(f"\n‚öôÔ∏è Configuring LoRA...")

lora_config = LoraConfig(
    r=4,  # 8 ‚Üí 4 (Îçî ÏûëÍ≤å)
    lora_alpha=8,  # 16 ‚Üí 8
    target_modules=["q_proj", "v_proj"],  # 2Í∞úÎßå
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ========================================
# 5. Prepare dataset
# ========================================

print(f"\nüîÑ Preparing dataset...")

def preprocess_function(example):
    full_text = example['input'] + "\n" + example['output'] + tokenizer.eos_token
    
    tokenized = tokenizer(
        full_text,
        truncation=True,
        max_length=MAX_LENGTH,  # 256
        padding="max_length"
    )
    
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

train_dataset = Dataset.from_list(train_data)
val_dataset = Dataset.from_list(val_data)

train_dataset = train_dataset.map(
    preprocess_function,
    remove_columns=train_dataset.column_names,
    desc="Processing train"
)

val_dataset = val_dataset.map(
    preprocess_function,
    remove_columns=val_dataset.column_names,
    desc="Processing val"
)

print(f"‚úÖ Dataset prepared!")

# ========================================
# 6. Training arguments
# ========================================

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    
    learning_rate=LEARNING_RATE,
    warmup_steps=20,  # ÏûëÍ≤å
    fp16=True,
    optim="adamw_torch",
    
    eval_strategy="steps",
    eval_steps=50,
    
    save_strategy="steps",
    save_steps=50,
    save_total_limit=2,
    
    logging_steps=10,
    logging_dir=f"{OUTPUT_DIR}/logs",
    report_to="none",
    
    dataloader_num_workers=0,
)

# ========================================
# 7. Trainer
# ========================================

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# ========================================
# 8. Train!
# ========================================

print("\n" + "="*70)
print("üöÄ Starting QUICK TEST training...")
print("="*70)

total_steps = len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION)

print(f"""
Configuration:
- Dataset: {len(train_data)} examples (SMALL)
- Epochs: {EPOCHS}
- Total steps: ~{total_steps}
- Max length: {MAX_LENGTH}

‚è±Ô∏è Estimated time: 20-30 minutes
""")

print("Starting in 3 seconds...")
import time
time.sleep(3)

try:
    start_time = time.time()
    
    trainer.train()
    
    elapsed = time.time() - start_time
    
    print("\n" + "="*70)
    print("‚úÖ Quick test complete!")
    print("="*70)
    print(f"‚è±Ô∏è Time taken: {elapsed/60:.1f} minutes")
    
    # Save
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"‚úÖ Test model saved to: {OUTPUT_DIR}")
    
    # Evaluate
    eval_results = trainer.evaluate()
    print(f"\nTest results:")
    for key, value in eval_results.items():
        print(f"  {key}: {value:.4f}")
    
    print("\n" + "="*70)
    print("üéâ Quick test done! Now ready for full training.")
    print("="*70)

except KeyboardInterrupt:
    print("\n‚ö†Ô∏è Interrupted")
    
except Exception as e:
    print(f"\n‚ùå Error: {e}")
    import traceback
    traceback.print_exc()
