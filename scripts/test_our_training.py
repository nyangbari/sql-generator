#!/usr/bin/env python3
# test_our_training.py
# ìš°ë¦¬ Fine-tuningì´ ì œëŒ€ë¡œ ëëŠ”ì§€ í™•ì¸

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

print("="*70)
print("ğŸ§ª ìš°ë¦¬ Fine-tuning ì§„ë‹¨")
print("="*70)

# ëª¨ë¸ ë¡œë“œ
print("\n1ï¸âƒ£ ëª¨ë¸ ë¡œë”©...")

base_model_id = "codellama/CodeLlama-7b-Instruct-hf"
tokenizer = AutoTokenizer.from_pretrained(base_model_id)

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True
)

model = PeftModel.from_pretrained(base_model, "./models/sql-generator-spider-plus-company")
model = model.merge_and_unload()

print("âœ… ì™„ë£Œ!")

# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤ (Spider í˜•ì‹ ê·¸ëŒ€ë¡œ)
test_cases = [
    {
        "name": "Spider í•™ìŠµ í˜•ì‹ ê·¸ëŒ€ë¡œ",
        "prompt": """# Given the database schema:
CREATE TABLE singer (
    singer_id INT PRIMARY KEY,
    name VARCHAR(100),
    country VARCHAR(50),
    age INT
)

# Question: How many singers are there?

# SQL:
"""
    },
    {
        "name": "ìš°ë¦¬ DB í˜•ì‹",
        "prompt": """# Given the database schema:
CREATE TABLE fury_mission_configs (
    missionId INT PRIMARY KEY,
    missionName VARCHAR(100),
    points INT
)

# Question: How many missions are there?

# SQL:
"""
    },
    {
        "name": "ê°„ë‹¨í•œ í˜•ì‹",
        "prompt": """Table: users (id, name, age)

Question: How many users?

SQL:"""
    }
]

print("\n" + "="*70)
print("ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œì‘")
print("="*70)

for i, test in enumerate(test_cases, 1):
    print(f"\n{'='*70}")
    print(f"í…ŒìŠ¤íŠ¸ {i}: {test['name']}")
    print(f"{'='*70}")
    
    print(f"\nğŸ“ í”„ë¡¬í”„íŠ¸:")
    print(test['prompt'])
    
    print("\nğŸ¤” ìƒì„± ì¤‘...")
    
    inputs = tokenizer(test['prompt'], return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # SQL ì¶”ì¶œ
    if "# SQL:" in result:
        sql = result.split("# SQL:")[-1].strip()
    elif "SQL:" in result:
        sql = result.split("SQL:")[-1].strip()
    else:
        sql = result.strip()
    
    sql = sql.split('\n')[0].strip()
    
    print(f"\nğŸ’¾ ìƒì„±ëœ SQL:")
    print(sql)
    
    # í‰ê°€
    has_select = "SELECT" in sql.upper()
    has_from = "FROM" in sql.upper()
    has_count = "COUNT" in sql.upper()
    
    score = sum([has_select, has_from, has_count])
    
    print(f"\nâœ… í‰ê°€:")
    print(f"   SELECT: {'âœ…' if has_select else 'âŒ'}")
    print(f"   FROM: {'âœ…' if has_from else 'âŒ'}")
    print(f"   COUNT(*): {'âœ…' if has_count else 'âŒ'}")
    print(f"   ì ìˆ˜: {score}/3 {'â­' * score}")

print("\n" + "="*70)
print("ğŸ’¡ ì§„ë‹¨ ê²°ê³¼")
print("="*70)

print("""
ë§Œì•½ Spider í˜•ì‹ì€ ì˜ ë˜ëŠ”ë° ë‹¤ë¥¸ í˜•ì‹ì€ ì•ˆ ë˜ë©´:
â†’ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë¬¸ì œ

ë§Œì•½ ë‹¤ ì•ˆ ë˜ë©´:
â†’ Fine-tuning ìì²´ ë¬¸ì œ (ì¬í•™ìŠµ í•„ìš”)

ë§Œì•½ ë‹¤ ì˜ ë˜ë©´:
â†’ LangChain í†µí•© ì‹œ ë¬¸ì œ
""")
