#!/usr/bin/env python3
# langchain_simple_chain.py
# í˜„ì—… ìŠ¤íƒ€ì¼: Agent ì—†ì´ Simple Chain

import os
import sys
import torch
from dotenv import load_dotenv
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel
from langchain_huggingface import HuggingFacePipeline
from langchain_community.utilities.sql_database import SQLDatabase
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from sqlalchemy import inspect

load_dotenv()

class SimpleSQLChain:
    """í˜„ì—… ìŠ¤íƒ€ì¼: Chain ê¸°ë°˜ SQL Bot"""
    
    def __init__(self, model_path):
        print("="*70)
        print("ðŸ¤– LangChain Simple Chain (í˜„ì—… ìŠ¤íƒ€ì¼)")
        print("   - Agent ì—†ìŒ")
        print("   - ì˜ˆì¸¡ ê°€ëŠ¥í•œ íë¦„")
        print("   - ë©ˆì¶¤ ë¬¸ì œ ì—†ìŒ")
        print("="*70)
        
        print("\nðŸ”„ ëª¨ë¸ ë¡œë”©...")
        
        base_model_id = "codellama/CodeLlama-7b-Instruct-hf"
        tokenizer = AutoTokenizer.from_pretrained(base_model_id)
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_id,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_8bit=True
        )
        
        model = PeftModel.from_pretrained(base_model, model_path)
        model = model.merge_and_unload()
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=150,
            temperature=0.1,
            return_full_text=False
        )
        
        self.llm = HuggingFacePipeline(pipeline=pipe)
        
        # DB ì„¤ì •
        self.databases = {}
        for proj in ["KNIGHTFURY", "FURYX"]:
            uri = os.getenv(f"{proj}_DB_URI")
            if uri:
                self.databases[proj.lower()] = uri.replace("mysql://", "mysql+pymysql://")
        
        print("\nðŸ“š í”„ë¡œì íŠ¸:", ', '.join(self.databases.keys()))
        print("="*70)
    
    def get_schema(self, db, table_names):
        """ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸°"""
        inspector = inspect(db._engine)
        
        schema = ""
        for table in table_names:
            columns = inspector.get_columns(table)
            pk = inspector.get_pk_constraint(table)
            pk_cols = pk.get('constrained_columns', [])
            
            schema += f"\nTable: {table}\nColumns:\n"
            for col in columns:
                pk_mark = " (PK)" if col['name'] in pk_cols else ""
                schema += f"  - {col['name']}: {col['type']}{pk_mark}\n"
        
        return schema
    
    def ask(self, project, question):
        """ì§ˆë¬¸ â†’ SQL â†’ ì‹¤í–‰ â†’ ë‹µë³€ (Chain)"""
        
        print("\n" + "="*70)
        print(f"ðŸ“‚ {project} | ðŸ’¬ {question}")
        print("="*70)
        
        uri = self.databases.get(project.lower())
        if not uri:
            print("âŒ í”„ë¡œì íŠ¸ ì—†ìŒ")
            return None
        
        try:
            # DB ì—°ê²°
            db = SQLDatabase.from_uri(uri, sample_rows_in_table_info=0)
            
            # ìŠ¤í‚¤ë§ˆ
            tables = db.get_usable_table_names()
            main_tables = ['fury_users'] if 'fury_users' in tables else tables[:1]
            schema = self.get_schema(db, main_tables)
            
            print(f"\nðŸ“‹ ìŠ¤í‚¤ë§ˆ:\n{schema}")
            
            # Step 1: SQL ìƒì„± Chain
            sql_prompt = PromptTemplate(
                input_variables=["schema", "question"],
                template="""Given this database schema:

{schema}

Generate a SQL query to answer: {question}

Return ONLY the SQL query, nothing else.

SQL:"""
            )
            
            sql_chain = LLMChain(llm=self.llm, prompt=sql_prompt)
            
            print("\nðŸ”„ Step 1: SQL ìƒì„± ì¤‘...")
            
            sql = sql_chain.run(schema=schema, question=question)
            
            # SQL ì •ë¦¬
            sql = sql.strip()
            sql = sql.replace('```sql', '').replace('```', '').strip()
            sql = sql.split('\n')[0] if '\n\n' in sql else sql
            
            # ë³´ì•ˆ ì²´í¬
            sql_upper = sql.upper()
            if any(kw in sql_upper for kw in ['INSERT', 'UPDATE', 'DELETE', 'DROP', 'CREATE', 'ALTER']):
                print("ðŸš« ìœ„í—˜í•œ SQL ì°¨ë‹¨!")
                return None
            
            print(f"\nðŸ’¾ ìƒì„±ëœ SQL:\n{sql}")
            
            # Step 2: SQL ì‹¤í–‰
            print("\nðŸ”„ Step 2: ì‹¤í–‰ ì¤‘...")
            result = db.run(sql)
            
            print(f"\nðŸ“Š DB ê²°ê³¼:\n{result}")
            
            # Step 3: ë‹µë³€ ìƒì„± Chain
            answer_prompt = PromptTemplate(
                input_variables=["question", "sql", "result"],
                template="""Question: {question}
SQL: {sql}
Result: {result}

Provide a natural language answer in one sentence.

Answer:"""
            )
            
            answer_chain = LLMChain(llm=self.llm, prompt=answer_prompt)
            
            print("\nðŸ”„ Step 3: ë‹µë³€ ìƒì„± ì¤‘...")
            
            answer = answer_chain.run(question=question, sql=sql, result=result)
            answer = answer.strip().split('\n')[0]
            
            print("\n" + "="*70)
            print(f"ðŸ’¡ ìµœì¢… ë‹µë³€:")
            print(answer)
            print("="*70)
            
            return answer
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}")
            print("="*70)
            import traceback
            traceback.print_exc()
            return None

# ì‹¤í–‰
if __name__ == "__main__":
    MODEL_PATH = "./models/sql-generator-spider-plus-company"
    
    bot = SimpleSQLChain(MODEL_PATH)
    
    if len(sys.argv) > 2:
        bot.ask(sys.argv[1], sys.argv[2])
    else:
        # í…ŒìŠ¤íŠ¸
        bot.ask("knightfury", "How many users are in fury_users?")
        print("\n")
        bot.ask("knightfury", "What networks exist in fury_users?")
