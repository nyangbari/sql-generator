#!/usr/bin/env python3
import os
import torch
from typing import Any, List, Optional
from dotenv import load_dotenv
from pydantic import Field

# HuggingFace & PEFT (vLLM ëŒ€ì‹  ì‚¬ìš©)
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel
from langchain_huggingface import HuggingFacePipeline

# LangChain ê´€ë ¨
from langchain_core.language_models.llms import LLM
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_community.agent_toolkits.sql.base import create_sql_agent

load_dotenv()

class LangChainAgentBot:
    def __init__(self, model_path):
        print("ğŸš€ ëª¨ë¸ ë¡œë”© ì‹œì‘ (RTX 4060 Ti ìµœì í™” ëª¨ë“œ)...")
        
        base_model_id = "codellama/CodeLlama-7b-Instruct-hf"
        
        # 1. í† í¬ë‚˜ì´ì € ë° ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (8-bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ í™•ë³´)
        tokenizer = AutoTokenizer.from_pretrained(base_model_id)
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_id,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_8bit=True  # 16GB VRAMì—ì„œ ê°€ì¥ ì•ˆì •ì ì¸ ëª¨ë“œ
        )
        
        # 2. LoRA ì–´ëŒ‘í„° ê²°í•©
        print(f"ğŸ“¦ ì–´ëŒ‘í„° ê²°í•© ì¤‘: {model_path}")
        model = PeftModel.from_pretrained(base_model, model_path)
        model = model.merge_and_unload() # ì†ë„ í–¥ìƒì„ ìœ„í•œ ë³‘í•©
        
        # 3. LangChainìš© íŒŒì´í”„ë¼ì¸ ìƒì„±
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=256,
            temperature=0.1,
            top_p=0.95,
            repetition_penalty=1.15,
            return_full_text=False
        )
        self.llm = HuggingFacePipeline(pipeline=pipe)
        
        # 4. DB ì—°ê²°
        uri = os.getenv("KNIGHTFURY_DB_URI", "").replace("mysql://", "mysql+pymysql://")
        self.db = SQLDatabase.from_uri(uri)
        print("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")

    def ask(self, question):
        try:
            # ì—ì´ì „íŠ¸ ìƒì„±
            agent = create_sql_agent(
                llm=self.llm,
                db=self.db,
                agent_type="zero-shot-react-description",
                verbose=True,
                handle_parsing_errors="Check your output format. If you have the final answer, use 'Final Answer:' prefix clearly."
            )
            print(f"\nğŸ” ì§ˆë¬¸: {question}")
            # í•œê¸€ ë‹µë³€ ìœ ë„
            prompt = (
                f"SQLì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. "
                f"ë°˜ë“œì‹œ 'Thought:', 'Action:', 'Final Answer:' í˜•ì‹ì„ ì—„ê²©íˆ ì§€ì¼œì•¼ í•©ë‹ˆë‹¤. "
                f"ì§ˆë¬¸: {question}"
            )
            result = agent.invoke({"input": prompt})
            print(f"\nğŸ’¡ ê²°ê³¼: {result.get('output')}")
        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")

if __name__ == "__main__":
    # ì ˆëŒ€ ê²½ë¡œ
    MODEL_PATH = "/home/dongsucat1/ai/sql-generator/models/sql-generator-spider-plus-company"
    
    bot = LangChainAgentBot(MODEL_PATH)
    bot.ask("ì‚¬ìš©ì í…Œì´ë¸”ì— ë“±ë¡ëœ ì „ì²´ ì‚¬ìš©ì ìˆ˜ëŠ” ëª‡ ëª…ì¸ê°€ìš”?")
