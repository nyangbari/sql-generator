#!/usr/bin/env python3
import os
from typing import Any, List, Optional
from dotenv import load_dotenv
from pydantic import Field

from vllm import LLM as VLLM_Model, SamplingParams
from vllm.lora.request import LoRARequest

from langchain_core.language_models.llms import LLM
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_community.agent_toolkits.sql.base import create_sql_agent

load_dotenv()

class ReadOnlySQLDatabase(SQLDatabase):
    WRITE_KEYWORDS = ['INSERT', 'UPDATE', 'DELETE', 'DROP', 'CREATE', 'ALTER', 'TRUNCATE', 'REPLACE', 'MERGE']
    def run(self, command: str, fetch: str = "all", **kwargs):
        sql_upper = command.upper().strip()
        for keyword in self.WRITE_KEYWORDS:
            if keyword in sql_upper: raise ValueError(f"ğŸš« {keyword} ê¶Œí•œ ì—†ìŒ")
        return super().run(command, fetch=fetch, **kwargs)

class VLLMWrapper(LLM):
    vllm_model: Any = Field(default=None, exclude=True)
    sampling_params: Any = Field(default=None, exclude=True)
    lora_request: Any = Field(default=None, exclude=True)

    def __init__(self, model_path: str, **kwargs):
        super().__init__(**kwargs)
        print("ğŸ”„ vLLM ë¡œë”© (RTX 4060 Ti 16GB ì „ìš© ìµœì í™”)...")
        
        # ì‹¤ì œ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ë² ì´ìŠ¤ ëª¨ë¸
        base_model = "codellama/CodeLlama-7b-Instruct-hf"

        self.vllm_model = VLLM_Model(
            model=base_model,
            enable_lora=True,
            max_lora_rank=64,
            tensor_parallel_size=1,
            # --- OOM í•´ê²°ì„ ìœ„í•œ ìµœì¢… ì„¤ì • ---
            gpu_memory_utilization=0.85, # 16GB ì¤‘ ì•½ 13.6GB ì‚¬ìš© ì˜ˆì•½
            max_model_len=512,           # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ 512ë¡œ ì œí•œ (ìºì‹œ ë©”ëª¨ë¦¬ ìµœì†Œí™”)
            enforce_eager=True,          # CUDA Graph ìƒì„± ë°©ì§€ (WSL ë©”ëª¨ë¦¬ ìŠ¤íŒŒì´í¬ ë°©ì§€)
            disable_custom_all_reduce=True,
            # -------------------------------
            dtype="float16"
        )
        
        self.lora_request = LoRARequest("sql_adapter", 1, model_path)
        self.sampling_params = SamplingParams(
            temperature=0.1,
            top_p=0.95,
            max_tokens=200,
            stop=["\n\n", "Observation:", "Thought:"]
        )
        print("âœ… vLLM ë¡œë“œ ì™„ë£Œ!")

    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs) -> str:
        outputs = self.vllm_model.generate([prompt], self.sampling_params, lora_request=self.lora_request)
        return outputs[0].outputs[0].text

    @property
    def _llm_type(self) -> str: return "vllm_lora_optimized"

class LangChainAgentBot:
    def __init__(self, model_path):
        self.llm = VLLMWrapper(model_path=model_path)
        db_uri = os.getenv("KNIGHTFURY_DB_URI", "").replace("mysql://", "mysql+pymysql://")
        self.db = ReadOnlySQLDatabase.from_uri(db_uri)

    def ask(self, question):
        try:
            agent = create_sql_agent(
                llm=self.llm, db=self.db, agent_type="zero-shot-react-description",
                verbose=True, handle_parsing_errors=True
            )
            print(f"\nğŸ¤” ì§ˆë¬¸: {question}")
            result = agent.invoke({"input": question})
            print(f"\nğŸ’¡ ë‹µë³€: {result.get('output')}")
        except Exception as e: print(f"âŒ ì—ëŸ¬: {e}")

if __name__ == "__main__":
    MODEL_PATH = "/home/dongsucat1/ai/sql-generator/models/sql-generator-spider-plus-company"
    bot = LangChainAgentBot(MODEL_PATH)
    bot.ask("ì „ì²´ ì‚¬ìš©ì ìˆ˜ëŠ” ëª‡ ëª…ì¸ê°€ìš”?")
